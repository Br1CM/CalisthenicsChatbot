{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35a9c9a",
   "metadata": {},
   "source": [
    "# Creación del modelo de chat\n",
    "\n",
    "En este notebook crearemos un RAG partiendo de una base de documentos intercalados entre webs de dominio accesible de manera gratuita y articulos de ciencia del deporte (también de dominio público)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10bc5934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import nest_asyncio\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import ollama \n",
    "from tavily import TavilyClient\n",
    "workpath = 'C:/Users/Legion/TFM/Tareas/CalistenIA'\n",
    "os.chdir(workpath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff164c4",
   "metadata": {},
   "source": [
    "### Crear vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c21df6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(docspath, vs_path):\n",
    "    with open(docspath, 'rb') as file:\n",
    "        all_docs = pickle.load(file)\n",
    "    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    embeddings = np.array(model.encode(all_docs))\n",
    "    dimension = embeddings.shape[1]  # Dimension of the embeddings\n",
    "    index = faiss.IndexFlatL2(dimension)  # L2 distance index\n",
    "\n",
    "    # Add embeddings to the index\n",
    "    index.add(embeddings)\n",
    "\n",
    "    # Save index to disk (optional)\n",
    "    faiss.write_index(index, vs_path)\n",
    "    print('vector store done!')\n",
    "    \n",
    "def load_vector_store(path):\n",
    "    return faiss.read_index(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42581db",
   "metadata": {},
   "source": [
    "Una vez tengamos el vector store, dependiendo del mensaje del usuario habrá que buscar documentos relevantes a este."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf18e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retreive_docs_vs(message, docspath, model, index):\n",
    "    '''\n",
    "    (!!!) Requiere que el modelo y la vector store estén declaradas de manera global (!!!)\n",
    "    '''\n",
    "    query_vector = model.encode([message])[0]  # Single vector\n",
    "    D, I = index.search(np.array([query_vector]), k=5)  # Search for top-5 closest vectors\n",
    "    with open(docspath, 'rb') as file:\n",
    "        all_docs = pickle.load(file)\n",
    "    # D contains the distances, I contains the indices of closest vectors\n",
    "    closest_docs = [all_docs[i] for i in I[0]]\n",
    "    return closest_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcd35ed",
   "metadata": {},
   "source": [
    "## Traer llama3.1 a local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49867634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DESCARGAR LLAMA (USAR UNA VEZ)\n",
    "ollama.pull('llama3.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fefa3c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky appears blue to us because of a phenomenon called scattering. When sunlight enters Earth's atmosphere, it encounters tiny molecules of gases such as nitrogen (N2) and oxygen (O2). These molecules scatter the light in all directions, but they scatter shorter (blue) wavelengths more than longer (red) wavelengths.\n",
      "\n",
      "This is known as Rayleigh scattering, named after the British physicist Lord Rayleigh who first described the phenomenon. As a result of this scattering, the blue light becomes distributed throughout the atmosphere and reaches our eyes from all directions, giving the sky its blue appearance.\n",
      "\n",
      "There are some interesting variations on this theme to note:\n",
      "\n",
      "1.  **At sunrise and sunset**, when sunlight passes through more of the Earth's atmosphere to reach your eyes, the shorter wavelengths are scattered away by the air molecules, leaving mainly longer wavelengths (like reds and oranges) to enter our eyes. This is why these times of day often display hues of orange, pink, or even purple.\n",
      "\n",
      "2.  **During the daytime**, when the sky appears blue, it's actually the same process of Rayleigh scattering that we discussed above that makes this happen. The color you see isn't the actual color of the light coming from the sun; it's just what happens to be left after all the other wavelengths have been scattered out.\n",
      "\n",
      "3.  **In space**, where there is no atmosphere, the sky would appear black because there are no air molecules to scatter the light in any way. However, if you were on a planet with an atmosphere similar to ours but lacking oxygen, the sky might appear red or yellowish due to scattering from other elements present in the atmosphere.\n",
      "\n",
      "4.  **In urban areas**, the sky can sometimes look gray or hazy because of air pollution and particulate matter in the atmosphere. This scatters light in a way that reduces its intensity, leading to the duller appearance we see.\n",
      "\n",
      "In conclusion, the color of the sky is a result of how sunlight interacts with the Earth's atmosphere, specifically through scattering by molecules and particles present within it.\n"
     ]
    }
   ],
   "source": [
    "# Prueba de uso\n",
    "response = ollama.chat(model='llama3.1', messages=[\n",
    "   {\n",
    "     'role': 'user',\n",
    "     'content': 'Why is the sky blue?',\n",
    "   },\n",
    " ])\n",
    "\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafa7501",
   "metadata": {},
   "source": [
    "#### Clasificar mensaje (deporte o no deporte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365e2072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear modelo para respuestas de clasificacion binaria en base a prompting\n",
    "modelfile='''\n",
    "FROM llama3.1\n",
    "SYSTEM You are a binary answerer. You can only answer 'yes' or 'no' to the questions you are receiving. No more details, just 'yes' or 'no'.\n",
    "'''\n",
    "\n",
    "ollama.create(model='grader', modelfile=modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbe96d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_classifier(message):\n",
    "    print('--------MESSAGE CLASSIFICATION STARTED---------')\n",
    "    response_message_class = ollama.chat(model='grader', messages=[\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content':\"You are a classifier. The goal of your answer is to determine if the user's message is related to sports training, or a completely different topic. Answer 'yes' if it could be interpreted as related to sports, 'no' otherwise.\"\n",
    "        },\n",
    "       {\n",
    "         'role': 'user',\n",
    "         'content': f''' Here is my message: {message} \\n\n",
    "     \n",
    "         Is it sports-related? or am I talking about other topic?\n",
    "         ''',\n",
    "       },\n",
    "        {'role': 'assistant',\n",
    "        'content': 'One word answer: '}\n",
    "     ])\n",
    "    answer = response_message_class['message']['content']\n",
    "    if 'yes' in answer.lower():\n",
    "        final_answer = True\n",
    "    elif 'no' in answer.lower():\n",
    "        final_answer = False\n",
    "    else:\n",
    "        print('Error en la clasificacion del mensaje. Respuesta LLM: '+answer)\n",
    "        final_answer = False\n",
    "    print('--------MESSAGE CLASSIFICATION DONE---------')\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ed4c4c",
   "metadata": {},
   "source": [
    "#### Message not related to sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b63b251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrong_topic_message(message):\n",
    "    print('--------WRONG TOPIC MESSAGE STARTED---------')\n",
    "    response_message = ollama.chat(model='grader', messages=[\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content':\"You are a sports chatbot. You have been asked for a topic that is not under the scope of sports. In this case, your main goal is to answer politely to the user saying that you can not provide information related to that topic. Suggest asking for something related to calisthenics, saying that you can help in that matter.\"\n",
    "        },\n",
    "       {\n",
    "         'role': 'user',\n",
    "         'content': f''' {message} \\n\n",
    "         ''',\n",
    "       },\n",
    "        {'role': 'assistant',\n",
    "        'content': 'Polite rejection of answer: '}\n",
    "     ])\n",
    "    answer = response_message['message']['content']    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4367bf2e",
   "metadata": {},
   "source": [
    "#### Grader documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90552bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def grade_document_relevance(message, docs):\n",
    "    print('--------DOCUMENT GRADER STARTED---------')\n",
    "    response_grade = ollama.chat(model='grader', messages=[\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content':'Give a binary answer to the question provided in the end of the text'\n",
    "        },\n",
    "       {\n",
    "         'role': 'user',\n",
    "         'content': f''' I have some documents. I am looking for an answer to my question in this documents. \\n\n",
    "         Here are the documents: \\n\\n {docs} \\n\\n\n",
    "         Here is the question I want to answer with this documents: {message} \\n\n",
    "     \n",
    "         Can I find the solution to my question in the documents?\n",
    "         Please answer 'yes' or 'no' only.\n",
    "         ''',\n",
    "       },\n",
    "        {'role': 'assistant',\n",
    "        'content': 'One word answer:'}\n",
    "     ])\n",
    "    \n",
    "    answer = response_grade['message']['content']\n",
    "    if 'yes' in answer.lower():\n",
    "        final_answer = True\n",
    "    elif 'no' in answer.lower():\n",
    "        final_answer = False\n",
    "    else:\n",
    "        print('Error en la clasificacion del mensaje. Respuesta LLM: '+answer)\n",
    "        final_answer = False\n",
    "    print('--------DOCUMENT GRADER DONE---------')\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a43d74",
   "metadata": {},
   "source": [
    "#### Answer generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f457e555",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_answer(message, docs):\n",
    "    print('--------ANSWER CREATION STARTED---------')\n",
    "    response_answer = ollama.chat(model='llama3.1', messages=[\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content':\"You are an assistant for question-answering tasks. Use the context given to answer the question. If you don't get the answer with the context, just say that you don't know. Use three sentences maximum and keep the answer concise.\"\n",
    "        },\n",
    "       {\n",
    "         'role': 'user',\n",
    "         'content': f''' Question: {message} \\n\n",
    "         Context: {docs} \\n\n",
    "         ''',\n",
    "       },{\n",
    "           'role': 'assistant',\n",
    "           'content': 'Answer:',\n",
    "       }\n",
    "     ])\n",
    "    print('--------ANSWER CREATION DONE---------')\n",
    "    return response_answer['message']['content']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65155eb3",
   "metadata": {},
   "source": [
    "#### Hallucination grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f679b14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def hallucination_grader(docs, answer):\n",
    "    print('--------HALLUCINATION GRADER STARTED---------')\n",
    "    response_hall_grade = ollama.chat(model='grader', messages=[\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content':\"You are a grader assesing wether an answer is grounded in / supported by a set of facts. provide de binary score as a single word 'yes' or 'no', and no preamble or explanation.\"\n",
    "        },\n",
    "       {\n",
    "         'role': 'user',\n",
    "         'content': f''' Here I have documents: {docs} \\n\n",
    "         Here is the answer I am giving from those facts: {answer} \\n\n",
    "         \n",
    "         Is my answer based in the facts of the document?\n",
    "         ''',\n",
    "       },\n",
    "        {'role': 'assistant',\n",
    "        'content': 'One word answer:'}\n",
    "     ])\n",
    "    response_bot = response_hall_grade['message']['content']\n",
    "    if 'yes' in response_bot.lower():\n",
    "        final_answer = True\n",
    "    elif 'no' in response_bot.lower():\n",
    "        final_answer = False\n",
    "    else:\n",
    "        print('Error en la clasificacion del mensaje. Respuesta LLM: '+response_bot)\n",
    "        final_answer = False\n",
    "    print('--------HALLUCINATION GRADER DONE---------')\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a6e0af",
   "metadata": {},
   "source": [
    "#### Check answer helpful for the message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed674a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'grader', 'created_at': '2024-08-14T09:22:27.1634162Z', 'message': {'role': 'assistant', 'content': ' {\"score\": \"yes\"}'}, 'done_reason': 'stop', 'done': True, 'total_duration': 10659204300, 'load_duration': 7946059000, 'prompt_eval_count': 178, 'prompt_eval_duration': 1318030000, 'eval_count': 7, 'eval_duration': 1368651000}\n"
     ]
    }
   ],
   "source": [
    "def check_question_answering(message, answer):\n",
    "    print('--------QA CHECK STARTED---------')\n",
    "    response_check_qa = ollama.chat(model='grader', messages=[\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content':\"You are a grader assesing wether an answer is helpful for a previous message. provide de binary score as a single word 'yes' or 'no', and no preamble or explanation.\"\n",
    "        },\n",
    "       {\n",
    "         'role': 'user',\n",
    "         'content': f''' Here I have my message: {message} \\n\n",
    "         Here is the answer I am receiving: {answer} \\n\n",
    "         \n",
    "         Is this answer actually helpful for my message?\n",
    "         ''',\n",
    "       },\n",
    "        {'role': 'assistant',\n",
    "        'content': 'One word score: '}\n",
    "     ])\n",
    "    response_bot = response_check_qa['message']['content']\n",
    "    if 'yes' in response_bot.lower():\n",
    "        final_answer = True\n",
    "    elif 'no' in response_bot.lower():\n",
    "        final_answer = False\n",
    "    else:\n",
    "        print('Error en la clasificacion del mensaje. Respuesta LLM: '+ response_bot)\n",
    "        final_answer = False\n",
    "    print('--------QA CHECK DONE---------')\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3548555",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '¿Cómo hacer una hamburguesa?', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'Cómo hacer hamburguesas caseras (con imágenes) - wikiHow', 'url': 'https://es.wikihow.com/hacer-hamburguesas-caseras', 'content': 'Cómo hacer hamburguesas caseras. ¡Tira a la basura esas hamburguesas de carne procesada rellena de grasa nada saludable que tienes! ... Para hacer una hamburguesa, compra carne molida de res con un 15 % de grasa y colócala en un tazón. Agrégale cebolla y ajo picados, y mezcla todos los ingredientes con una cuchara. Luego, casca 1 huevo ...', 'score': 0.9995661, 'raw_content': None}, {'title': 'Receta de Hamburguesa Clásica l Fácil y Rápida! - Easy Recetas', 'url': 'https://easyrecetas.com/receta/hamburguesa-clasica/', 'content': 'Cómo hacer hamburguesas clásica. 1. Precaliente la parrilla a 375 grados F (medio-alto). En un tazón grande, agregue la carne. Espolvoree uniformemente con la salsa inglesa, sal de sazonar, ajo en polvo y pimienta. Use sus manos para mezclar los ingredientes hasta que estén bien combinados. 2.', 'score': 0.99906373, 'raw_content': None}, {'title': 'Cómo hacer la hamburguesa perfecta según la receta del chef Michelin ...', 'url': 'https://www.directoalpaladar.com.mx/chefs/como-hacer-hamburguesa-perfecta-receta-chef-michelin-gordon-ramsay', 'content': 'Cómo hacer una hamburguesa perfecta según Gordon Ramsay. Dificultad: Fácil. Tiempo total 10 m; Elaboración 10 m; Mezcla las yemas de huevo en un recipiente con la carne molida. Espolvorea sal y pimienta negra recién molida y agrega el chile rayado y, con ayuda de tus manos, mezcla todo.', 'score': 0.99906003, 'raw_content': None}, {'title': 'Receta hamburguesas caseras de carne molida | Gourmet', 'url': 'https://www.gourmet.cl/recetas/hamburguesas-caseras/', 'content': 'Prepara tus propias hamburguesas caseras de carne molida y disfruta de una gran comida con amigos o familia. Conoce aquí cómo hacerlas paso a paso. ... ¿Cómo hacer Hamburguesas Caseras de Carne Molida? Ingredientes. 500 gr carne molida; ... En un sartén, calentar la mitad el aceite. Una vez que esté listo, agregar la cebolla picada y el ...', 'score': 0.9988796, 'raw_content': None}, {'title': 'Hamburguesas caseras, la mejor receta para triunfar - Directo al Paladar', 'url': 'https://www.directoalpaladar.com/recetas-de-carnes-y-aves/como-hacer-hamburguesas-caseras', 'content': 'Cómo hacer hamburguesas caseras perfectas: trucos, consejos y las mejores ideas para triunfar . 5 comentarios Facebook Twitter Flipboard E-mail. ... Para hacer una buena hamburguesa, los panes se ...', 'score': 0.998675, 'raw_content': None}], 'response_time': 12.94}\n"
     ]
    }
   ],
   "source": [
    "tavily = TavilyClient(api_key='-------API KEY--------')\n",
    "\n",
    "web_answer = tavily.search(query=question)\n",
    "print(web_answer)\n",
    "\n",
    "def tavily_doc_retrieval(tavily_prompt):\n",
    "    tavily = TavilyClient(api_key='-------API KEY--------')\n",
    "    web_answer = tavily.search(query=tavily_prompt)\n",
    "    doc = ''\n",
    "    for web_index in range(len(web_answer['results'])):\n",
    "        doc += '-------------START WEB PAGE------------------'\n",
    "        doc += '# WEB PAGE ' + str(web_index + 1) + ' \\n'\n",
    "        doc += '## URL: ' + web_answer['results'][web_index]['url'] + ' \\n'\n",
    "        doc += '### TITLE: ' + web_answer['results'][web_index]['title'] + ' \\n'\n",
    "        doc += '#### CONTENT: \\n ' web_answer['results'][web_index]['content'] + ' \\n'\n",
    "        doc += '-------------END WEB PAGE------------------'\n",
    "    return doc\n",
    "\n",
    "    \n",
    "def tavily_answering(message):\n",
    "    if len(message) < 140:\n",
    "        doc = tavily_doc_retrieval(message)\n",
    "        websearch_answer = ollama.chat(model='llama3.1', messages=[\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content':\"\"\"You are a content summarizer. You get documents based on webpages search, your goal is to extract the key information from the webpages' document.\n",
    "                give an answer to the question based in the webpages'document. \n",
    "                Provide the URLS in the answer so the user can search more information.\n",
    "                It is important to state in the answer that this is an answer based in web search, so there could be some mistakes in the given information.\n",
    "                Encourage the user to verify the information.\n",
    "                \"\"\"\n",
    "            },\n",
    "           {\n",
    "             'role': 'user',\n",
    "             'content': f''' Question: {message} \\n\n",
    "             Webpages' documents: {doc} \\n\n",
    "             ''',\n",
    "           },{\n",
    "               'role': 'assistant',\n",
    "               'content': 'Answer:',\n",
    "           }\n",
    "         ])\n",
    "    else: \n",
    "        response_prompt = ollama.chat(model='llama3.1', messages=[\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content':\"\"\"You are a question summarizer. You get a message from the user, your goal is to extract the key information to create a short question that sums up what the user is looking for.\n",
    "            The question will be used to search on the internet for an answer. Make the question efficient for a proper google search\n",
    "                \"\"\"\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': f''' Question: {message} \\n\n",
    "            ''',\n",
    "        },{\n",
    "            'role': 'assistant',\n",
    "            'content': 'Summarized One sentence question:',\n",
    "            }\n",
    "        ])\n",
    "        doc = tavily_doc_retrieval(response_prompt['message']['content'])\n",
    "        websearch_answer = ollama.chat(model='llama3.1', messages=[\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content':\"\"\"You are a content summarizer. You get documents based on webpages search, your goal is to extract the key information from the webpages' document.\n",
    "                give an answer to the question based in the webpages'document. \n",
    "                Provide the URLS in the answer so the user can search more information.\n",
    "                It is important to state in the answer that this is an answer based in web search, so there could be some mistakes in the given information.\n",
    "                Encourage the user to verify the information.\n",
    "                \"\"\"\n",
    "            },\n",
    "           {\n",
    "             'role': 'user',\n",
    "             'content': f''' Question: {message} \\n\n",
    "             Webpages' documents: {doc} \\n\n",
    "             ''',\n",
    "           },{\n",
    "               'role': 'assistant',\n",
    "               'content': 'Answer:',\n",
    "           }\n",
    "         ])\n",
    "    print('--------ONLINE ANSWER CREATED---------')\n",
    "    return websearch_answer['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117d8919",
   "metadata": {},
   "source": [
    "## Juntar los pasos en un LLM Chaining "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68433420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_qa(user_prompt, model, index):\n",
    "    sports_related = message_classifier(user_prompt)\n",
    "    if sports_related:\n",
    "        related_doc = retreive_docs_vs(user_prompt, './Models/RAG/all_docs.pickle', model, index)\n",
    "        doc_related_to_prompt = grade_document_relevance(user_prompt, related_doc)\n",
    "        if doc_related_to_prompt:\n",
    "            answer_prompt = create_answer(user_prompt, related_doc)\n",
    "            answer_related_to_docs = hallucination_grader(related_doc, answer_prompt)\n",
    "            if answer_related_to_docs:\n",
    "                answers_user_question = check_question_answering(user_prompt, answer_prompt)\n",
    "                if answers_user_question:\n",
    "                    user_answer = answer_prompt\n",
    "                else:\n",
    "                    user_answer = tavily_answering(user_prompt)\n",
    "            else:\n",
    "                user_answer = tavily_answering(user_prompt)\n",
    "        else:\n",
    "            user_answer = tavily_answering(user_prompt)\n",
    "    else:\n",
    "        user_answer = wrong_topic_message(user_prompt)\n",
    "        \n",
    "    return user_answer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
