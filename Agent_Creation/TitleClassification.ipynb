{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a599c055-bc8d-4c11-a484-fb363926ea65",
   "metadata": {},
   "source": [
    "# Title Classification\n",
    "\n",
    "En este documento se empezará a crear la base de datos que se utilizará para la vector store de nuestro RAG para el chatbot.\n",
    "\n",
    "Teniendo una lista de +19500 ariculos relacionados con la ciencia del deporte, debemos atacar los que puedan ser relevantes para nuestro chatbot en un principio, evitando tener muchos documentos que puedan no servirnos para nuestro propósito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c45a36c8-26b0-4db8-a8b2-7c044b28a7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import os\n",
    "workpath = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025898a9-a1b7-45f3-8ab4-b973befa1718",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cargar el pipeline de clasificación zero-shot\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Lista de posibles clases\n",
    "candidate_labels = [\"calisthenics\", \"excercise\", \"posture\", \"nutrition\", \"training\", \"posture\", \"massage\", \"recovery\"]\n",
    "\n",
    "# Sacar titulos\n",
    "data = pd.read_csv('./Docs/EstudiosTotal.csv', sep=',').drop_duplicates(subset='Title')\n",
    "titulos = data['Title'].to_list()\n",
    "clase = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd44b21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "candidate_labels = [\"calisthenics\", \"excercise\", \"posture\", \"nutrition\",\n",
    "                    \"training\", \"posture\", \"massage\", \"recovery\", \"biomechanics\"]\n",
    "# Clasificar los títulos\n",
    "for titulo_index in range(len(titulos)):\n",
    "    resultado = classifier(titulos[titulo_index], candidate_labels)\n",
    "    clase.append(resultado['labels'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6fc3af9-6f4a-45ed-b55a-b6810a251980",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Tipo'] = clase[:15450]\n",
    "data.to_csv('./Docs/EstudiosTotalClasificadoBien.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30dcee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['Tipo'].isin(['excercise', 'posture', 'calisthenics', 'training'])].to_csv('./Docs/EstudiosEntrenamiento.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfa00d2",
   "metadata": {},
   "source": [
    "## Automatizar descarga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370bcfad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "def load_urls(df_path):\n",
    "    df = pd.read_csv(df_path, sep=',')\n",
    "    urls = df.loc[df['Label']==1]['URL'].tolist()\n",
    "    return [url.strip() for url in urls]\n",
    "\n",
    "def fetch_pdf_link(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    pdf_link = None\n",
    "    for link in soup.find_all('a'):\n",
    "        if 'pdf' in link.get('href', '').lower():\n",
    "            pdf_link = link['href']\n",
    "            break\n",
    "    try:\n",
    "        return url+pdf_link\n",
    "    except:\n",
    "        return pdf_link\n",
    "def download_pdf(pdf_url, output_dir='pdfs'):\n",
    "    try:\n",
    "        response = requests.get(pdf_url)\n",
    "        pdf_name = pdf_url.split('/')[-1]\n",
    "        with open(f'{output_di}/{pdf_name}', 'wb') as file:\n",
    "            file.write(response.rcontent)\n",
    "    except:\n",
    "        print('No se pudo descargar')\n",
    "# Load URLs from file\n",
    "urls = load_urls(workpath+'/Docs/PruebaArticulos.csv')\n",
    "\n",
    "# Fetch PDF links\n",
    "pdf_links = [fetch_pdf_link(url) for url in urls]\n",
    "print(pdf_links)\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs('pdfs', exist_ok=True)\n",
    "\n",
    "# Download PDFs\n",
    "for pdf_link in pdf_links:\n",
    "    if pdf_link is not None:\n",
    "        download_pdf(pdf_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fc5221",
   "metadata": {},
   "source": [
    "**Importante:** La descarga no se pudo automatizar, por lo que tras la clasificación de los distintos documentos en base a su título se verifican y descargan de manera manual aquellos que se consideren más relevantes. No habrá tanta cantidad de archivos pero al menos será información útil."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
